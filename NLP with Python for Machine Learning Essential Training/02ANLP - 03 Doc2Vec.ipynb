{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec: How too implement it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So instead of creating a vector for each word, this technique will create a vector for each document or collection of text, whether it's a sentence or a paragraph. The goal is the same as `word2vec`. To create a numeric representation of a set of texts to feed to Python to help it better understand the meaning. \n",
    "\n",
    "Recall that `word2vec` is a shallow two-layer neural network that accepts a text corpus as an input, and it returns a set of vectors, also known as embeddings. Each vector is a numeric representation of a given word. `doc2vec` is basically the same thing, but instead of returning a numeric vector for each word, it returns a numeric vector for each sentence or paragraph.\n",
    "\n",
    "The real benefit of `Doc2Vec` is it captures information about a sentence or paragraph, which is what we need, in a much more sophisticated way than creating word vectors and then averaging them. So in `Word2Vec`, we lose information by averaging the word vectors together to create a sentence or text level representation. `Doc2Vec` is able to capture the sentence level representation in a much more sophisticated way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Our Own Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lsoares\\.ipython\\profile_default\n"
     ]
    }
   ],
   "source": [
    "!ipython locate profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, only, in, bugis, great, world, la, buffet, cine, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[ok, lar, joking, wif, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[free, entry, in, wkly, comp, to, win, fa, cup, final, tkts, st, may, text, fa, to, to, receive,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[dun, say, so, early, hor, already, then, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[nah, don, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...   \n",
       "1                                                                        Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "3                                                    U dun say so early hor... U c already then say...   \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                            text_clean  \n",
       "0  [go, until, jurong, point, crazy, available, only, in, bugis, great, world, la, buffet, cine, th...  \n",
       "1                                                                          [ok, lar, joking, wif, oni]  \n",
       "2  [free, entry, in, wkly, comp, to, win, fa, cup, final, tkts, st, may, text, fa, to, to, receive,...  \n",
       "3                                                       [dun, say, so, early, hor, already, then, say]  \n",
       "4                                [nah, don, think, he, goes, to, usf, he, lives, around, here, though]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data, clean it, and then split into train and test sets\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "messages = pd.read_csv('data/spam.csv', encoding='latin-1')\n",
    "messages = messages.drop(labels = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis = 1)\n",
    "messages.columns = [\"label\", \"text\"]\n",
    "messages['text_clean'] = messages['text'].apply(lambda x: gensim.utils.simple_preprocess(x))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(messages['text_clean'],\n",
    "                                                    messages['label'], \n",
    "                                                    test_size=0.2)\n",
    "\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, one of the differences between `word2vec` and `doc2vec` is that `doc2vec` requires you to create tagged documents. This tagged document, expects a list of words and a tag for each document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to iterate through X_train using this enumerate function and that'll return the index and the value for each text message in X_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['am', 'in', 'hospital', 'da', 'will', 'return', 'home', 'in', 'evening'], tags=[0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tagged document objects to prepare to train the model\n",
    "tagged_docs = [gensim.models.doc2vec.TaggedDocument(v, [i]) for i, v in enumerate(X_train)]\n",
    "\n",
    "# Look at what a tagged document looks like\n",
    "tagged_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a basic doc2vec model\n",
    "d2v_model = gensim.models.Doc2Vec(tagged_docs,     # Tagged documents for training\n",
    "                                 vector_size=100,  # Dimensionality of the document vectors\n",
    "                                 window=5,         # Maximum distance between the current and predicted word within a sentence\n",
    "                                 min_count=2)      # Minimum number of occurrences of a word to be considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01278291,  0.01385637,  0.00353282, -0.00238422,  0.00467299,\n",
       "       -0.03675859,  0.00250603,  0.04687937, -0.02790658, -0.01388085,\n",
       "       -0.03651403, -0.02648865, -0.00297569, -0.00889337,  0.01047053,\n",
       "       -0.04358591, -0.0002554 , -0.0226766 , -0.00327277, -0.04131157,\n",
       "        0.02080492,  0.00489327,  0.02491716,  0.00317137,  0.00695801,\n",
       "        0.0137176 , -0.02586616,  0.00102074, -0.00716305,  0.00327264,\n",
       "        0.02259056,  0.00783179,  0.00463704,  0.01325294,  0.00277061,\n",
       "        0.03034823,  0.00244458, -0.01216242, -0.01260882, -0.02291754,\n",
       "        0.00689542, -0.00736155, -0.01786701, -0.02653614,  0.01008196,\n",
       "       -0.02338689, -0.02427665, -0.00778264,  0.00328216,  0.0140456 ,\n",
       "        0.00458468, -0.00955786,  0.00651383,  0.01409424,  0.00259051,\n",
       "        0.01095745, -0.00033736, -0.01037658, -0.01538919,  0.01542132,\n",
       "       -0.00360482,  0.00156963, -0.00743088,  0.00562836, -0.03563394,\n",
       "        0.02587644,  0.01428414,  0.00813987, -0.02170091,  0.02963761,\n",
       "       -0.01249314,  0.01961257,  0.01481239, -0.01232184, -0.00256669,\n",
       "        0.02335882, -0.00253566,  0.00482461, -0.02167666,  0.01360132,\n",
       "       -0.00405452, -0.00042594, -0.0041646 ,  0.04183706, -0.01847571,\n",
       "        0.01383955,  0.00222048,  0.01145078,  0.0166446 ,  0.01053384,\n",
       "        0.02306463,  0.01388336, -0.009713  ,  0.0038846 ,  0.03217745,\n",
       "        0.00963072,  0.00280195, -0.02194516,  0.0088758 ,  0.01763171],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What happens if we pass in a single word like we did for word2vec?\n",
    "d2v_model.infer_vector(['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00195068,  0.00776231,  0.0048783 , -0.00510567, -0.0045562 ,\n",
       "       -0.02515473,  0.00220668,  0.03511856, -0.01456716, -0.01292632,\n",
       "       -0.016941  , -0.02226035, -0.00354946,  0.00380918,  0.00120318,\n",
       "       -0.02651529,  0.00421664, -0.01306386, -0.01014609, -0.03112501,\n",
       "        0.01729575,  0.00504408,  0.01124415, -0.0078456 , -0.00469393,\n",
       "        0.00120825, -0.01017751, -0.00550199, -0.01298617,  0.00108419,\n",
       "        0.01511465,  0.00391247,  0.01086077, -0.00812416, -0.00496949,\n",
       "        0.02287463,  0.00999818, -0.01166083, -0.01518472, -0.0243851 ,\n",
       "       -0.00221257, -0.01534109, -0.00935461, -0.00781237,  0.01012304,\n",
       "       -0.00980569, -0.01169394, -0.00061955,  0.0102037 ,  0.01977467,\n",
       "        0.00984916, -0.00073883,  0.00279152,  0.00264358, -0.00433003,\n",
       "        0.00853682,  0.00952507, -0.00676516, -0.01401376,  0.0082838 ,\n",
       "       -0.00380021,  0.00097366, -0.00064311,  0.00248695, -0.02615652,\n",
       "        0.02211107,  0.00652581,  0.01287649, -0.0216535 ,  0.01966092,\n",
       "       -0.00625675,  0.01617701,  0.02124689, -0.0010804 ,  0.01745566,\n",
       "        0.01053593, -0.00150772, -0.00565301, -0.01307418,  0.00286285,\n",
       "       -0.00799742,  0.00344422, -0.01430051,  0.03269583, -0.01564283,\n",
       "        0.00239588,  0.00155401,  0.01154131,  0.01679974,  0.00596671,\n",
       "        0.0199728 ,  0.01667866,  0.0021313 ,  0.01354741,  0.03337612,\n",
       "        0.00688437,  0.00090691, -0.00491146,  0.00609115,  0.00683019],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What happens if we pass in a list of words?\n",
    "d2v_model.infer_vector(['i','am','learning','nlp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What About Pre-trained Document Vectors?\n",
    "\n",
    "There are not as many options as there are for word vectors. There also is not an easy API to read these in like there is for `word2vec` so it is more time consuming.\n",
    "\n",
    "Pre-trained vectors from training on Wikipedia and Associated Press News can be found [here](https://github.com/jhlau/doc2vec). Feel free to explore on your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How To Prep Document Vectors For Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data, clean it, split it into train/test, and then train a doc2vec model\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "\n",
    "messages = pd.read_csv('data/spam.csv', encoding='latin-1')\n",
    "messages = messages.drop(labels = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis = 1)\n",
    "messages.columns = [\"label\", \"text\"]\n",
    "messages['text_clean'] = messages['text'].apply(lambda x: gensim.utils.simple_preprocess(x))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(messages['text_clean'],\n",
    "                                                    messages['label'], \n",
    "                                                    test_size=0.2)\n",
    "\n",
    "\n",
    "# Create tagged document objects to prepare to train the model\n",
    "tagged_docs_tr = [gensim.models.doc2vec.TaggedDocument(v, [i]) for i, v in enumerate(X_train)]\n",
    "\n",
    "d2v_model = gensim.models.Doc2Vec(tagged_docs_tr,\n",
    "                                  vector_size=50,\n",
    "                                  window=2,\n",
    "                                  min_count=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
